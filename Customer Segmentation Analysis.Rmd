---
title: "Customer Segmentation Using Machine Learning"
author: "Dory Munoko"
date: "12/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

we will make use of K-means clustering which is the essential algorithm for clustering unlabeled dataset.

1. Import packages and read data.
```{r}
install.packages("plotrix")
install.packages("NbClust")
install.packages("factoextra")
```

```{r}
library(plotrix)
library(cluster)
```


```{r}
customer_data <- read.csv("/Users/ruthb/Desktop/Dory's Projects/Customer Segmentation Project/customer-segmentation-dataset/Mall_Customers.csv")
View(customer_data)
```

```{r}
str(customer_data)
```

```{r}
names(customer_data)
```

```{r}
head(customer_data)
```

```{r}
tail(customer_data)
```

```{r}
summary(customer_data)
```

```{r}
summary(customer_data$Age)
```

```{r}
sd(customer_data$Age)
```

```{r}
summary(customer_data$Annual.Income..k..)
```

```{r}
sd(customer_data$Annual.Income..k..)
```

```{r}
summary(customer_data$Spending.Score..1.100.)
```

```{r}
sd(customer_data$Spending.Score..1.100.)
```

2. Customer Gender Visualization
Gender distribution using bar plot and pie chart.
```{r}
a=table(customer_data$Gender)
barplot(a,main="Bar Plot Showing Gender Comparision",
       ylab="Count",
       xlab="Gender",
       c(1,1), axes=FALSE, col=c("red","darkblue"),
       legend=rownames(a))
```

Ratio of male and female distribution using a pie chart.
```{r}
pct=round(a/sum(a)*100)
lbs=paste(c("Female","Male")," ",pct,"%",sep=" ")
pie3D(a,labels=lbs,
      col=c("red","darkblue"),
   main="Pie Chart Showing Ratio of Female and Male")
```

3. Visualization of Age Distribution
Plot a histogram to view the distribution to plot the frequency of customer ages.
```{r}
hist(customer_data$Age,
    col="darkred",
    main="Histogram Showing Count of Age Class",
    xlab="Age Class",
    ylab="Frequency",
    labels=TRUE)
```
Descriptive analysis of age using boxplot. 
The maximum customer ages are between 30 and 35. The minimum age of customers is 18, and the maximum age is 70.
```{r}
boxplot(customer_data$Age,
       col="darkred",
       main="Boxplot for Descriptive Analysis of Age")
```

4. Analysis of the Annual Income of the Customers
Plot histogram to analyze annual income of customers.
```{r}
hist(customer_data$Annual.Income..k..,
  col="darkred",
  main="Histogram Showing Annual Income",
  xlab="Annual Income Class",
  ylab="Frequency",
  labels=TRUE)
```

Density plot for annual income
We observe that the annual income has a normal distribution. Maximum and minimum annual income are 137 and 15. Average salary is 60.56. Earners of an average income of 70 have the highest frequency count.
```{r}
plot(density(customer_data$Annual.Income..k..),
    col="green",
    main="Density Plot for Annual Income",
    xlab="Annual Income Class",
    ylab="Density")
polygon(density(customer_data$Annual.Income..k..),
        col="green")
```

5. Analyzing Spending Score of the Customers
BoxPlot for descriptive analysis of spending score
We observe that the minimum is 1 and maximum is 99, while the average is 50.20.
```{r}
boxplot(customer_data$Spending.Score..1.100.,
        horizontal = TRUE,
        col = "darkred",
        main = "BoxPlot Showing Spending Score")

```

Histogram showing spending score.
Customers between class 40 and 50 have the highest spending score among all the classes.
```{r}
hist(customer_data$Spending.Score..1.100.,
     main = "Histogram Showing Spending Score",
     xlab = "Spending Score Class",
     ylab = "Frequency",
     col = "darkred",
     labels = TRUE)
```

6. K-means Clustering Algorithm
Methods to Determine Optimal Clusters:
a.) Elbow Method
We conclude that 4 is the appropriate number of clusters since it seems to be appearing at the bend in the elbow plot.
```{r}
library(purrr)
set.seed(123)
# function to calculate total intra-cluster sum of square 
iss <- function(k) {
  kmeans(customer_data[,3:5],k,iter.max=100,nstart=100,algorithm="Lloyd" )$tot.withinss
}
k.values <- 1:10
iss_values <- map_dbl(k.values, iss)
plot(k.values, iss_values,
    type="b", pch = 19, frame = FALSE, 
    xlab="Number of clusters K",
    ylab="Total intra-clusters sum of squares")
```

b.) Average Silhouette Method
Determines how well within the cluster is the data object.
A high average silhouette width means that we have good clustering.
```{r}
library(cluster) 
library(gridExtra)
library(grid)
k2<-kmeans(customer_data[,3:5],2,iter.max=100,nstart=50,algorithm="Lloyd")
s2<-plot(silhouette(k2$cluster,dist(customer_data[,3:5],"euclidean")))
```

```{r}
k3<-kmeans(customer_data[,3:5],3,iter.max=100,nstart=50,algorithm="Lloyd")
s3<-plot(silhouette(k3$cluster,dist(customer_data[,3:5],"euclidean")))
```

```{r}
k4<-kmeans(customer_data[,3:5],4,iter.max=100,nstart=50,algorithm="Lloyd")
s4<-plot(silhouette(k4$cluster,dist(customer_data[,3:5],"euclidean")))
```

```{r}
k5<-kmeans(customer_data[,3:5],5,iter.max=100,nstart=50,algorithm="Lloyd")
s5<-plot(silhouette(k5$cluster,dist(customer_data[,3:5],"euclidean")))
```

```{r}
k6<-kmeans(customer_data[,3:5],6,iter.max=100,nstart=50,algorithm="Lloyd")
s6<-plot(silhouette(k6$cluster,dist(customer_data[,3:5],"euclidean")))
```

```{r}
k7<-kmeans(customer_data[,3:5],7,iter.max=100,nstart=50,algorithm="Lloyd")
s7<-plot(silhouette(k7$cluster,dist(customer_data[,3:5],"euclidean")))
```

```{r}
k8<-kmeans(customer_data[,3:5],8,iter.max=100,nstart=50,algorithm="Lloyd")
s8<-plot(silhouette(k8$cluster,dist(customer_data[,3:5],"euclidean")))
```

```{r}
k9<-kmeans(customer_data[,3:5],9,iter.max=100,nstart=50,algorithm="Lloyd")
s9<-plot(silhouette(k9$cluster,dist(customer_data[,3:5],"euclidean")))
```

```{r}
k10<-kmeans(customer_data[,3:5],10,iter.max=100,nstart=50,algorithm="Lloyd")
s10<-plot(silhouette(k10$cluster,dist(customer_data[,3:5],"euclidean")))
```

Use fviz_nbclust() function to determine and visualize the optimal number of clusters
```{r}
library(NbClust)
library(factoextra)
fviz_nbclust(customer_data[,3:5], kmeans, method = "silhouette")
```

c.) Gap Statistic Method
Compare the total intracluster variation for different values of k along with their expected values under the null reference distribution of data. 
Calculate the range between min(xi) and max (xj) for each variable, then produce values uniformly from interval lower bound to upper bound.
```{r}
set.seed(125)
stat_gap <- clusGap(customer_data[,3:5], FUN = kmeans, nstart = 25,
            K.max = 10, B = 50)
fviz_gap_stat(stat_gap)
```

k6 as the optimal cluster
```{r}
k6<-kmeans(customer_data[,3:5],6,iter.max=100,nstart=50,algorithm="Lloyd")
k6
```

Visualizing the Clustering Results using the First Two Principle Components
```{r}
pcclust=prcomp(customer_data[,3:5],scale=FALSE) 
summary(pcclust)
pcclust$rotation[,1:2]
```

```{r}
set.seed(1)
ggplot(customer_data, aes(x =Annual.Income..k.., y = Spending.Score..1.100.)) + 
  geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
  scale_color_discrete(name=" ",
              breaks=c("1", "2", "3", "4", "5","6"),
              labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Segments of Mall Customers", subtitle = "Using K-means Clustering")
```

Cluster 1 – High annual income as well as a high annual spend.
Cluster 2 – High annual income and low yearly spend.
Cluster 3 – Low annual income as well as low yearly spend of income
Cluster 5 – Low annual income but its high yearly expenditure.
Cluster 4 & 6 – Medium income salary as well as the medium annual spend of salary.

```{r}
ggplot(customer_data, aes(x =Spending.Score..1.100., y =Age)) + 
  geom_point(stat = "identity", aes(color = as.factor(k6$cluster))) +
  scale_color_discrete(name=" ",
                      breaks=c("1", "2", "3", "4", "5","6"),
                      labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Segments of Mall Customers", subtitle = "Using K-means Clustering")
```

```{r}
kCols=function(vec){cols=rainbow (length (unique (vec)))
return (cols[as.numeric(as.factor(vec))])}

digCluster<-k6$cluster; dignm<-as.character(digCluster); # K-means clusters

plot(pcclust$x[,1:2], col =kCols(digCluster),pch =19,xlab ="K-means",ylab="classes")

legend("bottomleft",unique(dignm),fill=unique(kCols(digCluster)))

```

Cluster 1 & 4 – Customers with medium PCA1 and medium PCA2 score.
Cluster 2 – Customers with a high PCA2 and a medium annual spend of income.
Cluster 3 – Customers with a high PCA1 income and a high PCA2.
Cluster 5 – Customers with a medium PCA1 and a low PCA2 score.
Cluster 6 – Customers having a high PCA2 and a low PCA1.










